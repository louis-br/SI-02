acuracia: 0.8422222222222222

0.8418797042957467

[[ 46  20   0   0]
 [  8 228  22   0]
 [  0  13 100   5]
 [  0   0   3   5]]

activation: tanh
alpha: 0.0001
batch_size: auto
beta_1: 0.9
beta_2: 0.999
early_stopping: False
epsilon: 1e-08
hidden_layer_sizes: (200, 100, 50)
learning_rate: constant
learning_rate_init: 0.0001
max_fun: 15000
max_iter: 2000
momentum: 0.9
n_iter_no_change: 15
nesterovs_momentum: True
power_t: 0.5
random_state: None
shuffle: True
solver: adam
tol: 1e-05
validation_fraction: 0.1
verbose: 1
warm_start: False
